{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2578a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import lxml\n",
    "import time\n",
    "STATE_IDS = range(1, 37)\n",
    "MAX_PAGES = 1000\n",
    "BASE_URL = \"https://www.myneta.info/LokSabha2024/index.php?action=show_constituencies&state_id={state_id}&page={page_num}\"\n",
    "OUTPUT_FILENAME = \"lok_sabha_2024_candidates_full.csv\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Referer': 'https://www.myneta.info/LokSabha2024/'\n",
    "}\n",
    "def extract_table_data(soup, state_name):\n",
    "    main_table = soup.find('table', class_='w3-table w3-bordered')\n",
    "    if not main_table:\n",
    "        return None, None\n",
    "    header_row = main_table.find('tr')\n",
    "    if header_row:\n",
    "        headers = [th.get_text(strip=True) for th in header_row.find_all('th')]\n",
    "    else:\n",
    "        headers = ['Sno.', 'Candidate', 'Party', 'Criminal Cases', 'Education', 'Age', 'Total Assets', 'Liabilities']\n",
    "    headers = [\"State\"] + headers + [\"Detail_Link\"]\n",
    "    data_rows = []\n",
    "    for row in main_table.find_all('tr')[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        if not cols:\n",
    "            continue\n",
    "        row_data = [state_name]\n",
    "        detail_link = None\n",
    "        for i, col in enumerate(cols):\n",
    "            text_content = col.get_text(strip=True).replace('\\n', ' ').strip()\n",
    "            if i == 1:\n",
    "                link_tag = col.find('a', href=True)\n",
    "                if link_tag:\n",
    "                    detail_link = link_tag['href']\n",
    "            if col.find('img'):\n",
    "                text_content = \"Data in Image\"\n",
    "            elif 'Rs' in text_content and '~' in text_content:\n",
    "                text_content = text_content.split('~')[0].strip()\n",
    "            row_data.append(text_content)\n",
    "        row_data.append(detail_link)\n",
    "        data_rows.append(row_data)\n",
    "    return data_rows, headers\n",
    "\n",
    "def scrape_all_states(state_ids):\n",
    "    all_data = []\n",
    "    master_headers = None\n",
    "\n",
    "    for state_id in state_ids:\n",
    "        page_num = 1\n",
    "\n",
    "        while page_num <= MAX_PAGES:\n",
    "            url = BASE_URL.format(state_id=state_id, page_num=page_num)\n",
    "            if page_num == 1 or page_num % 10 == 0 or page_num == MAX_PAGES:\n",
    "                 print(f\"-> Scraping State ID {state_id}, Page {page_num}...\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(url, headers=HEADERS, timeout=20)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.content, 'lxml')\n",
    "                state_name_tag = soup.find('div', class_='w3-panel w3-leftbar w3-sand')\n",
    "                state_name = \"Unknown State\"\n",
    "                if state_name_tag:\n",
    "                    state_name_text = state_name_tag.get_text(strip=True)\n",
    "                    if '(' in state_name_text and ')' in state_name_text:\n",
    "                        state_name = state_name_text.split('(')[-1].split(')')[0]\n",
    "\n",
    "                current_data, headers = extract_table_data(soup, state_name)\n",
    "\n",
    "                if current_data:\n",
    "                    all_data.extend(current_data)\n",
    "                    if master_headers is None:\n",
    "                        master_headers = headers\n",
    "\n",
    "                    page_num += 1\n",
    "                    time.sleep(1)\n",
    "                else:\n",
    "                    print(f\"   Finished State ID {state_id} ({state_name}) at Page {page_num - 1}. Stopping pagination.\")\n",
    "                    break\n",
    "\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                print(f\"   HTTP Error for State ID {state_id} Page {page_num}: {e}. Stopping pagination.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"   An unexpected error occurred for State ID {state_id} Page {page_num}: {e}. Stopping.\")\n",
    "                break\n",
    "        time.sleep(5)\n",
    "\n",
    "    return all_data, master_headers\n",
    "start_time = time.time()\n",
    "full_data, final_headers = scrape_all_states(STATE_IDS)\n",
    "end_time = time.time()\n",
    "\n",
    "if full_data:\n",
    "    if final_headers:\n",
    "        max_cols = max(len(row) for row in full_data)\n",
    "        if len(final_headers) > max_cols:\n",
    "            final_headers = final_headers[:max_cols]\n",
    "        elif len(final_headers) < max_cols:\n",
    "            final_headers.extend([f\"Extra_Col_{i}\" for i in range(len(final_headers), max_cols)])\n",
    "        df = pd.DataFrame(full_data, columns=final_headers)\n",
    "        df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
    "    else:\n",
    "        print(\"Data collected, but failed to determine headers for final CSV.\")\n",
    "else:\n",
    "    print(\"\\n Failed to collect any data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c92eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import lxml\n",
    "import time\n",
    "\n",
    "URL = \"https://www.myneta.info/LokSabha2024/index.php?action=recontestAssetsComparison\"\n",
    "OUTPUT_FILENAME = \"lok_sabha_2024_asset_comparison.csv\"\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "}\n",
    "\n",
    "def scrape_asset_comparison(url, headers):\n",
    "    \"\"\"\n",
    "    Fetches the HTML, locates the asset comparison table, and extracts all data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"ðŸŒ Fetching data from: {url}\")\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "        main_table = soup.find('table', class_='w3-table w3-bordered')\n",
    "\n",
    "        if not main_table:\n",
    "            main_table = soup.find('table')\n",
    "            if not main_table:\n",
    "                print(\"Error: Could not find the main data table on the page.\")\n",
    "                return None, None\n",
    "\n",
    "        headers = [th.get_text(strip=True) for th in main_table.find('tr').find_all('th')]\n",
    "\n",
    "        data_rows = []\n",
    "        for row in main_table.find_all('tr')[1:]:\n",
    "            cols = row.find_all('td')\n",
    "            if not cols:\n",
    "                continue\n",
    "\n",
    "            row_data = []\n",
    "\n",
    "            for i, col in enumerate(cols):\n",
    "                text_content = col.get_text(strip=True).replace('\\n', ' ').strip()\n",
    "\n",
    "                if 'Rs' in text_content and '~' in text_content:\n",
    "                    text_content = text_content.split('~')[0].strip()\n",
    "\n",
    "                if col.find('img'):\n",
    "                    text_content = \"Data in Image\"\n",
    "\n",
    "                if i == 1:\n",
    "                    row_data.append(text_content)\n",
    "                else:\n",
    "                    row_data.append(text_content)\n",
    "\n",
    "            data_rows.append(row_data)\n",
    "\n",
    "        return data_rows, headers\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred during the web request: {e}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# --- Main Execution ---\n",
    "data, headers = scrape_asset_comparison(URL, HEADERS)\n",
    "\n",
    "if data and headers:\n",
    "    print(f\"\\nSuccessfully extracted {len(data)} data row(s).\")\n",
    "\n",
    "    num_cols_in_data = len(data[0]) if data else 0\n",
    "    df = pd.DataFrame(data, columns=headers[:num_cols_in_data])\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    df.to_csv(OUTPUT_FILENAME, index=False, encoding='utf-8')\n",
    "    print(f\"ðŸ’¾ Data saved successfully to **{OUTPUT_FILENAME}**\")\n",
    "    print(\"\\n--- DataFrame Head ---\")\n",
    "    print(df.head().to_markdown(index=False))\n",
    "else:\n",
    "    print(\"\\n Scraping failed. No data to process.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
